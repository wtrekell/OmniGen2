{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from pathlib import Path\n",
    "root_dir = Path().resolve()\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2 import OmniGen2Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collage(images: List[torch.Tensor]) -> Image.Image:\n",
    "    \"\"\"Create a horizontal collage from a list of images.\"\"\"\n",
    "    max_height = max(img.shape[-2] for img in images)\n",
    "    total_width = sum(img.shape[-1] for img in images)\n",
    "    canvas = torch.zeros((3, max_height, total_width), device=images[0].device)\n",
    "    \n",
    "    current_x = 0\n",
    "    for img in images:\n",
    "        h, w = img.shape[-2:]\n",
    "        canvas[:, :h, current_x:current_x+w] = img * 0.5 + 0.5\n",
    "        current_x += w\n",
    "    \n",
    "    return to_pil_image(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_image_path: List[str] = []) -> Tuple[str, str, List[Image.Image]]:\n",
    "    \"\"\"Preprocess the input images.\"\"\"\n",
    "    # Process input images\n",
    "    input_images = []\n",
    "\n",
    "    if input_image_path:\n",
    "        if isinstance(input_image_path, str):\n",
    "            input_image_path = [input_image_path]\n",
    "            \n",
    "        if len(input_image_path) == 1 and os.path.isdir(input_image_path[0]):\n",
    "            input_images = [Image.open(os.path.join(input_image_path[0], f)) \n",
    "                          for f in os.listdir(input_image_path[0])]\n",
    "        else:\n",
    "            input_images = [Image.open(path) for path in input_image_path]\n",
    "\n",
    "        input_images = [ImageOps.exif_transpose(img) for img in input_images]\n",
    "\n",
    "    return input_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "model_path=\"OmniGen2/OmniGen2\"\n",
    "pipeline = OmniGen2Pipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    token=\"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\",\n",
    ")\n",
    "pipeline = pipeline.to(accelerator.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text to image generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar\"\n",
    "\n",
    "instructions = [\n",
    "    \"The sun rises slightly, the dew on the rose petals in the garden is clear, a crystal ladybug is crawling to the dew, the background is the early morning garden, macro lens.\",\n",
    "    \"Hyperrealistic macro photograph of a whimsical rabbit sculpture, meticulously crafted from an assortment of fresh garden vegetables. Its body is formed from crisp lettuce and cabbage leaves, with vibrant carrot slices for ears, bright red radish for eyes, and delicate parsley sprigs for fur. The rabbit is sitting on a rustic, dark wood cutting board, with a few scattered water droplets glistening on its surface. Dramatic, warm studio lighting from the side casts soft shadows, highlighting the intricate textures of the vegetables. Shallow depth of field, sharp focus, cinematic food photography, 8K, bokeh background.\",\n",
    "]\n",
    "for instruction in instructions:\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "    results = pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=[],\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        num_inference_steps=50,\n",
    "        max_sequence_length=1024,\n",
    "        text_guidance_scale=4.0,\n",
    "        image_guidance_scale=1.0,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=1,\n",
    "        generator=generator,\n",
    "        output_type=\"pil\",\n",
    "    )\n",
    "\n",
    "    vis_images = [to_tensor(image) * 2 - 1 for image in results.images]\n",
    "    output_image = create_collage(vis_images)\n",
    "\n",
    "    display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Editing with instruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar\"\n",
    "\n",
    "inputs = [\n",
    "    (\"Change the background to classroom.\", \"example_images/ComfyUI_temp_mllvz_00071_.png\"),\n",
    "    (\"Generate a photo of an anime-style figurine placed on a desk. The figurine model should be based on the character photo provided in the attachment, accurately replicating the full-body pose, facial expression, and clothing style of the character in the photo, ensuring the entire figurine is fully presented. The overall design should be exquisite and detailed, soft gradient colors and a delicate texture, leaning towards a Japanese anime style, rich in details, with a realistic quality and beautiful visual appeal.\", \"example_images/RAL_0315.JPG\"),\n",
    "]\n",
    "\n",
    "for instruction, input_image in inputs:\n",
    "    input_images = preprocess(input_image)\n",
    "\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "    results = pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=input_images,\n",
    "        num_inference_steps=50,\n",
    "        max_sequence_length=1024,\n",
    "        text_guidance_scale=5.0,\n",
    "        image_guidance_scale=2.0,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=1,\n",
    "        generator=generator,\n",
    "        output_type=\"pil\",\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(results.images) + len(input_images), figsize=(results.images[0].width / results.images[0].height * 5 * (len(results.images) + len(input_images)), 5))\n",
    "\n",
    "    for i, input_image in enumerate(input_images):\n",
    "        axes[i].imshow(input_image)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Input {i+1}')\n",
    "    \n",
    "    for i, output_image in enumerate(results.images):\n",
    "        axes[len(input_images) + i].imshow(output_image)\n",
    "        axes[len(input_images) + i].axis('off')\n",
    "        axes[len(input_images) + i].set_title(f'Output {i+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-context Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar\"\n",
    "\n",
    "inputs = [\n",
    "    (\"Please let the person in image 2 hold the toy from the first image in a parking lot.\", [\"example_images/04.jpg\", \"example_images/000365954.jpg\"]),\n",
    "    (\"Add the bird from image 1 to the desk in image 2.\", [\"example_images/996e2cf6-daa5-48c4-9ad7-0719af640c17_1748848108409.png\", \"example_images/00066-10350085.png\"]),\n",
    "]\n",
    "\n",
    "for instruction, input_images in inputs:\n",
    "    input_images = preprocess(input_images)\n",
    "\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "    results = pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=input_images,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        num_inference_steps=50,\n",
    "        max_sequence_length=1024,\n",
    "        text_guidance_scale=5.0,\n",
    "        image_guidance_scale=2.0,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=1,\n",
    "        generator=generator,\n",
    "        output_type=\"pil\",\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(results.images) + len(input_images), figsize=(results.images[0].width / results.images[0].height * 5 * (len(results.images) + len(input_images)), 5))\n",
    "\n",
    "    for i, input_image in enumerate(input_images):\n",
    "        axes[i].imshow(input_image)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Input {i+1}')\n",
    "    \n",
    "    for i, output_image in enumerate(results.images):\n",
    "        axes[len(input_images) + i].imshow(output_image)\n",
    "        axes[len(input_images) + i].axis('off')\n",
    "        axes[len(input_images) + i].set_title(f'Output {i+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2_chat import OmniGen2ChatPipeline\n",
    "\n",
    "chat_pipeline = OmniGen2ChatPipeline.from_pipe(pipeline=pipeline, transformer=pipeline.transformer)\n",
    "\n",
    "inputs = [\n",
    "    (\"Please briefly describe this image.\", \"example_images/04.jpg\"),\n",
    "    (\"Could you tell me the color of the woman's hat in the picture?\", \"example_images/000077066.jpg\"),\n",
    "]\n",
    "\n",
    "for instruction, input_image in inputs:\n",
    "    input_images = preprocess(input_image)\n",
    "\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "\n",
    "    results = chat_pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=input_images,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    # !! Uncomment following lines to visualize the input images\n",
    "    print(\"Input Image:\")\n",
    "    vis_images = [to_tensor(image) * 2 - 1 for image in input_images]\n",
    "    input_images = create_collage(vis_images)\n",
    "    display(input_images)\n",
    "    print(\"Output Text:\")\n",
    "    print(results.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
