{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from pathlib import Path\n",
    "root_dir = Path().resolve()\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2 import OmniGen2Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collage(images: List[torch.Tensor]) -> Image.Image:\n",
    "    \"\"\"Create a horizontal collage from a list of images.\"\"\"\n",
    "    max_height = max(img.shape[-2] for img in images)\n",
    "    total_width = sum(img.shape[-1] for img in images)\n",
    "    canvas = torch.zeros((3, max_height, total_width), device=images[0].device)\n",
    "    \n",
    "    current_x = 0\n",
    "    for img in images:\n",
    "        h, w = img.shape[-2:]\n",
    "        canvas[:, :h, current_x:current_x+w] = img * 0.5 + 0.5\n",
    "        current_x += w\n",
    "    \n",
    "    return to_pil_image(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_image_path: List[str] = []) -> Tuple[str, str, List[Image.Image]]:\n",
    "    \"\"\"Preprocess the input images.\"\"\"\n",
    "    # Process input images\n",
    "    input_images = []\n",
    "\n",
    "    if input_image_path:\n",
    "        if isinstance(input_image_path, str):\n",
    "            input_image_path = [input_image_path]\n",
    "            \n",
    "        if len(input_image_path) == 1 and os.path.isdir(input_image_path[0]):\n",
    "            input_images = [Image.open(os.path.join(input_image_path[0], f)) \n",
    "                          for f in os.listdir(input_image_path[0])]\n",
    "        else:\n",
    "            input_images = [Image.open(path) for path in input_image_path]\n",
    "\n",
    "    return input_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "model_path=\"OmniGen2/OmniGen2\"\n",
    "pipeline = OmniGen2Pipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    token=\"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\",\n",
    ")\n",
    "pipeline = pipeline.to(accelerator.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text to image generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\n",
    "\n",
    "instructions = [\n",
    "    \"A curly-haired man in a red shirt is drinking tea.\",\n",
    "    \"Hyperrealistic macro photograph of a whimsical rabbit sculpture, meticulously crafted from an assortment of fresh garden vegetables. Its body is formed from crisp lettuce and cabbage leaves, with vibrant carrot slices for ears, bright red radish for eyes, and delicate parsley sprigs for fur. The rabbit is sitting on a rustic, dark wood cutting board, with a few scattered water droplets glistening on its surface. Dramatic, warm studio lighting from the side casts soft shadows, highlighting the intricate textures of the vegetables. Shallow depth of field, sharp focus, cinematic food photography, 8K, bokeh background.\",\n",
    "]\n",
    "for instruction in instructions:\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "    results = pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=[],\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        num_inference_steps=28,\n",
    "        max_sequence_length=1024,\n",
    "        text_guidance_scale=5.0,\n",
    "        image_guidance_scale=1.0,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=3,\n",
    "        generator=generator,\n",
    "        output_type=\"pil\",\n",
    "    )\n",
    "\n",
    "    vis_images = [to_tensor(image) * 2 - 1 for image in results.images]\n",
    "    output_image = create_collage(vis_images)\n",
    "\n",
    "    display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Editing with instruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\n",
    "\n",
    "inputs = [\n",
    "    (\"Change the background to classroom.\", \"example_images/ComfyUI_temp_mllvz_00071_.png\"),\n",
    "]\n",
    "\n",
    "for instruction, input_image in inputs:\n",
    "    input_images = preprocess(input_image)\n",
    "\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "    results = pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=input_images,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        num_inference_steps=28,\n",
    "        max_sequence_length=1024,\n",
    "        text_guidance_scale=5.0,\n",
    "        image_guidance_scale=1.8,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=3,\n",
    "        generator=generator,\n",
    "        output_type=\"pil\",\n",
    "    )\n",
    "    # !! Uncomment following lines to visualize the input images\n",
    "    # vis_images = [to_tensor(image) * 2 - 1 for image in input_images]\n",
    "    # input_images = create_collage(vis_images)\n",
    "    # display(input_images)\n",
    "\n",
    "    vis_images = [to_tensor(image) * 2 - 1 for image in results.images]\n",
    "    output_image = create_collage(vis_images)\n",
    "    display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-context Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\n",
    "\n",
    "inputs = [\n",
    "    (\"Please let the person in image 2 hold the toy from the first image in a parking lot.\", [\"example_images/04.jpg\", \"example_images/000365954.jpg\"]),\n",
    "    (\"Please let the person in image 2 hold the toy from the first image in a parking lot.\", [\"example_images/04.jpg\", \"example_images/000365954.jpg\"]),\n",
    "]\n",
    "\n",
    "for instruction, input_images in inputs:\n",
    "    input_images = preprocess(input_images)\n",
    "\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "    results = pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=input_images,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        num_inference_steps=28,\n",
    "        max_sequence_length=1024,\n",
    "        text_guidance_scale=5.0,\n",
    "        image_guidance_scale=1.8,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=3,\n",
    "        generator=generator,\n",
    "        output_type=\"pil\",\n",
    "    )\n",
    "\n",
    "    # !! Uncomment following lines to visualize the input images\n",
    "    # vis_images = [to_tensor(image) * 2 - 1 for image in input_images]\n",
    "    # input_images = create_collage(vis_images)\n",
    "    # display(input_images)\n",
    "\n",
    "    vis_images = [to_tensor(image) * 2 - 1 for image in results.images]\n",
    "    output_image = create_collage(vis_images)\n",
    "\n",
    "    display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigen2.pipelines.omnigen2.pipeline_omnigen2_chat import OmniGen2ChatPipeline\n",
    "\n",
    "chat_pipeline = OmniGen2ChatPipeline.from_pipe(pipeline=pipeline)\n",
    "\n",
    "inputs = [\n",
    "    (\"Please briefly describe this image.\", \"example_images/04.jpg\"),\n",
    "    (\"Could you tell me the color of the woman's hat in the picture?\", \"example_images/000077066.jpg\"),\n",
    "]\n",
    "\n",
    "for instruction, input_image in inputs:\n",
    "    input_images = preprocess(input_image)\n",
    "\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(0)\n",
    "\n",
    "    results = chat_pipeline(\n",
    "        prompt=instruction,\n",
    "        input_images=input_images,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    print(results.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
